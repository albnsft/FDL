C:\Users\alban\PycharmProjects\FDL\venv\lib\site-packages\torchvision\transforms\transforms.py:329: UserWarning: Argument 'interpolation' of type int is deprecated since 0.13 and will be removed in 0.15. Please use InterpolationMode enum.
  warnings.warn(
===============================================================================================
Layer (type:depth-idx)                        Output Shape              Param #
===============================================================================================
├─MobileNetV2Encoder: 1-1                     [-1, 3, 192, 160]         --
|    └─Sequential: 2                          []                        --
|    |    └─Conv2dNormActivation: 3-1         [-1, 32, 96, 80]          928
|    |    └─InvertedResidual: 3-2             [-1, 16, 96, 80]          896
|    |    └─InvertedResidual: 3-3             [-1, 24, 48, 40]          5,136
|    |    └─InvertedResidual: 3-4             [-1, 24, 48, 40]          8,832
|    |    └─InvertedResidual: 3-5             [-1, 32, 24, 20]          10,000
|    |    └─InvertedResidual: 3-6             [-1, 32, 24, 20]          14,848
|    |    └─InvertedResidual: 3-7             [-1, 32, 24, 20]          14,848
|    |    └─InvertedResidual: 3-8             [-1, 64, 12, 10]          21,056
|    |    └─InvertedResidual: 3-9             [-1, 64, 12, 10]          54,272
|    |    └─InvertedResidual: 3-10            [-1, 64, 12, 10]          54,272
|    |    └─InvertedResidual: 3-11            [-1, 64, 12, 10]          54,272
|    |    └─InvertedResidual: 3-12            [-1, 96, 12, 10]          66,624
|    |    └─InvertedResidual: 3-13            [-1, 96, 12, 10]          118,272
|    |    └─InvertedResidual: 3-14            [-1, 96, 12, 10]          118,272
|    |    └─InvertedResidual: 3-15            [-1, 160, 6, 5]           155,264
|    |    └─InvertedResidual: 3-16            [-1, 160, 6, 5]           320,000
|    |    └─InvertedResidual: 3-17            [-1, 160, 6, 5]           320,000
|    |    └─InvertedResidual: 3-18            [-1, 320, 6, 5]           473,920
|    |    └─Conv2dNormActivation: 3-19        [-1, 1280, 6, 5]          412,160
├─FPNDecoder: 1-2                             [-1, 128, 48, 40]         --
|    └─Conv2d: 2-1                            [-1, 256, 6, 5]           327,936
|    └─FPNBlock: 2-2                          [-1, 256, 12, 10]         --
|    |    └─Conv2d: 3-20                      [-1, 256, 12, 10]         24,832
|    └─FPNBlock: 2-3                          [-1, 256, 24, 20]         --
|    |    └─Conv2d: 3-21                      [-1, 256, 24, 20]         8,448
|    └─FPNBlock: 2-4                          [-1, 256, 48, 40]         --
|    |    └─Conv2d: 3-22                      [-1, 256, 48, 40]         6,400
|    └─ModuleList: 2                          []                        --
|    |    └─SegmentationBlock: 3-23           [-1, 128, 48, 40]         590,592
|    |    └─SegmentationBlock: 3-24           [-1, 128, 48, 40]         442,880
|    |    └─SegmentationBlock: 3-25           [-1, 128, 48, 40]         295,168
|    |    └─SegmentationBlock: 3-26           [-1, 128, 48, 40]         295,168
|    └─MergeBlock: 2-5                        [-1, 128, 48, 40]         --
|    └─Dropout2d: 2-6                         [-1, 128, 48, 40]         --
├─SegmentationHead: 1-3                       [-1, 25, 192, 160]        --
|    └─Conv2d: 2-7                            [-1, 25, 48, 40]          3,225
|    └─UpsamplingBilinear2d: 2-8              [-1, 25, 192, 160]        --
|    └─Activation: 2-9                        [-1, 25, 192, 160]        --
|    |    └─Identity: 3-27                    [-1, 25, 192, 160]        --
===============================================================================================
Total params: 4,218,521
Trainable params: 4,218,521
Non-trainable params: 0
Total mult-adds (M): 65.06
===============================================================================================
Input size (MB): 0.35
Forward/backward pass size (MB): 9.68
Params size (MB): 16.09
Estimated Total Size (MB): 26.13
===============================================================================================
Epoch [1] took 79.51s | train_loss: 0.7290, train_acc: 0.2727, val_loss: 0.7360, val_acc: 0.2640
Epoch [2] took 79.32s | train_loss: 0.7099, train_acc: 0.2901, val_loss: 0.7360, val_acc: 0.2640
Epoch [3] took 79.73s | train_loss: 0.7097, train_acc: 0.2903, val_loss: 0.7360, val_acc: 0.2640
Epoch [4] took 79.18s | train_loss: 0.7150, train_acc: 0.2850, val_loss: 0.7360, val_acc: 0.2640
Epoch [5] took 79.24s | train_loss: 0.7100, train_acc: 0.2900, val_loss: 0.7360, val_acc: 0.2640
Epoch [6] took 78.41s | train_loss: 0.7096, train_acc: 0.2904, val_loss: 0.7360, val_acc: 0.2640
Epoch [7] took 79.99s | train_loss: 0.7124, train_acc: 0.2876, val_loss: 0.7360, val_acc: 0.2640
Epoch [8] took 80.91s | train_loss: 0.7103, train_acc: 0.2897, val_loss: 0.7360, val_acc: 0.2640
Epoch [9] took 81.81s | train_loss: 0.7117, train_acc: 0.2883, val_loss: 0.7360, val_acc: 0.2640
Epoch [10] took 80.00s | train_loss: 0.7123, train_acc: 0.2877, val_loss: 0.7360, val_acc: 0.2640
Traceback (most recent call last):
  File "C:\Users\alban\PycharmProjects\FDL\main.py", line 881, in <module>
    main_hyperopt(model, encoder, param)
  File "C:\Users\alban\PycharmProjects\FDL\main.py", line 858, in main_hyperopt
    HyperOptNet.main(model=model, param=param, name=name)
  File "C:\Users\alban\PycharmProjects\FDL\main.py", line 738, in main
    pred_mask_test = model.predict(data.test_dataloader)
  File "C:\Users\alban\PycharmProjects\FDL\main.py", line 583, in predict
    F.interpolate(pred_mask, (original_size[0], original_size[1])))
  File "C:\Users\alban\PycharmProjects\FDL\venv\lib\site-packages\torch\nn\functional.py", line 3866, in interpolate
    raise ValueError(
ValueError: Input and output must have the same number of spatial dimensions, but got input with spatial dimensions of [160] and output size of (4000, 3000). Please provide input tensor in (N, C, d1, d2, ...,dK) format and output size in (o1, o2, ...,oK) format.
Traceback (most recent call last):
  File "C:\Users\alban\PycharmProjects\FDL\main.py", line 881, in <module>
    main_hyperopt(model, encoder, param)
  File "C:\Users\alban\PycharmProjects\FDL\main.py", line 858, in main_hyperopt
    HyperOptNet.main(model=model, param=param, name=name)
  File "C:\Users\alban\PycharmProjects\FDL\main.py", line 738, in main
    pred_mask_test = model.predict(data.test_dataloader)
  File "C:\Users\alban\PycharmProjects\FDL\main.py", line 583, in predict
    F.interpolate(pred_mask, (original_size[0], original_size[1])))
  File "C:\Users\alban\PycharmProjects\FDL\venv\lib\site-packages\torch\nn\functional.py", line 3866, in interpolate
    raise ValueError(
ValueError: Input and output must have the same number of spatial dimensions, but got input with spatial dimensions of [160] and output size of (4000, 3000). Please provide input tensor in (N, C, d1, d2, ...,dK) format and output size in (o1, o2, ...,oK) format.